# PyTorchæ­£ç¡®æ€§éªŒè¯æµ‹è¯• - å®Œæ•´æŒ‡å—

æœ¬é¡¹ç›®æä¾›äº†å®Œæ•´çš„PyTorchéªŒè¯æµ‹è¯•ç³»ç»Ÿï¼Œç”¨äºéªŒè¯tensor_cppä¸­æ‰€æœ‰ç®—å­çš„æ­£ç¡®æ€§ã€‚

## ğŸ“‹ éªŒè¯ç³»ç»Ÿæ¦‚è¿°

### å·²å®ç°çš„éªŒè¯æµ‹è¯•

| ç®—å­ç±»å‹ | æµ‹è¯•æ–‡ä»¶ | éªŒè¯æ–¹æ³• | çŠ¶æ€ |
|---------|---------|---------|------|
| **Self-Attention** | torch_validation.py | F.scaled_dot_product_attention | âœ… å·²å®ç° |
| **Cross-Attention** | torch_validation.py | æ‰‹åŠ¨å®ç° | âœ… å·²å®ç° |
| **Streaming Attention** | torch_validation.py | æ‰‹åŠ¨å®ç° | âœ… å·²å®ç° |
| **Linear Layer** | torch_validation.py | torch.nn.functional.linear | âœ… å·²å®ç° |
| **RMS Norm** | torch_validation.py | è‡ªå®šä¹‰å®ç° | âœ… å·²å®ç° |
| **Embedding** | torch_validation.py | torch.nn.functional.embedding | âœ… å·²å®ç° |
| **Argmax** | torch_validation.py | torch.argmax | âœ… å·²å®ç° |
| **SwiGLU** | torch_validation.py | torch.nn.functional.silu | âœ… å·²å®ç° |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: å¿«é€ŸéªŒè¯æµ‹è¯• (æ¨èæ–°æ‰‹)

```bash
# æ­¥éª¤1: ç”Ÿæˆæµ‹è¯•æ•°æ®
python3 quick_attention_test.py

# æ­¥éª¤2: æŸ¥çœ‹ç”Ÿæˆçš„æµ‹è¯•æ•°æ®
ls -lh test_*.npy test_cross_*.npy

# æ­¥éª¤3: C++ç¨‹åºåŠ è½½è¿™äº›æ•°æ®è¿›è¡Œè®¡ç®—ï¼ˆéœ€è¦å®ç°ï¼‰
# ç„¶åç”Ÿæˆ cpp_self_attention_output.npy å’Œ cpp_cross_attention_output.npy

# æ­¥éª¤4: å†æ¬¡è¿è¡ŒéªŒè¯
python3 quick_attention_test.py
```

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
============================================================
  Quick Attention Validation Test
============================================================

Testing Self-Attention
============================================================
Input shape: (2, 2, 8, 16)
Reference output shape: (2, 2, 8, 16)
Reference output (first element): -0.126617
âœ“ Test data saved

Checking C++ Outputs
============================================================
Self-Attention:
  âœ“ PASSED - Max abs error: 1.23e-06
```

### æ–¹å¼2: å®Œæ•´éªŒè¯æµ‹è¯•

```bash
# ä¸€é”®è¿è¡Œæ‰€æœ‰æµ‹è¯•
./run_torch_validation.sh

# æˆ–æ‰‹åŠ¨åˆ†æ­¥æ‰§è¡Œ
python3 torch_validation.py                    # ç”Ÿæˆ18ä¸ªæµ‹è¯•ç”¨ä¾‹
cd tensor_cpp && make torch-validation          # ç¼–è¯‘
./build/torch_validation                        # è¿è¡ŒC++æµ‹è¯•
cd .. && python3 torch_validation.py --check-results  # éªŒè¯ç»“æœ
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
.
â”œâ”€â”€ torch_validation.py              # ä¸»éªŒè¯è„šæœ¬ï¼ˆ18ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
â”œâ”€â”€ quick_attention_test.py          # å¿«é€ŸéªŒè¯è„šæœ¬ï¼ˆ2ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼‰
â”œâ”€â”€ run_torch_validation.sh          # ä¸€é”®è¿è¡Œè„šæœ¬
â”œâ”€â”€ PYTORCH_VALIDATION_README.md     # è¯¦ç»†æ–‡æ¡£
â”œâ”€â”€ tensor_cpp/
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ torch_validation.cpp    # C++éªŒè¯ç¨‹åº
â”‚   â””â”€â”€ Makefile                    # æ·»åŠ äº†torch-validationç›®æ ‡
â””â”€â”€ test_data/                       # æµ‹è¯•æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
    â”œâ”€â”€ self_attention_1_*.npy
    â”œâ”€â”€ self_attention_1_meta.json
    â”œâ”€â”€ ...
    â””â”€â”€ index.json
```

## ğŸ¯ æµ‹è¯•è¦†ç›–è¯¦æƒ…

### Attentionç®—å­æµ‹è¯•

#### 1. Self-Attention (3ä¸ªæµ‹è¯•)
- ä½¿ç”¨ `torch.nn.functional.scaled_dot_product_attention`
- æµ‹è¯•é…ç½®ï¼š
  - å°: (batch=2, heads=2, seq=8, dim=16)
  - ä¸­: (batch=1, heads=4, seq=16, dim=32)
  - å¤§: (batch=4, heads=8, seq=64, dim=64)

#### 2. Cross-Attention (2ä¸ªæµ‹è¯•)
- éªŒè¯ä¸åŒåºåˆ—é•¿åº¦çš„äº¤å‰æ³¨æ„åŠ›
- æµ‹è¯•é…ç½®ï¼š
  - (batch=2, heads=2, q_len=8, kv_len=16, dim=16)
  - (batch=1, heads=4, q_len=32, kv_len=128, dim=32)

#### 3. Streaming Attention (3ä¸ªæµ‹è¯•)
- å•æŸ¥è¯¢æ ¼å¼çš„æµå¼æ³¨æ„åŠ›
- æµ‹è¯•é…ç½®ï¼š
  - T=512, d=64
  - T=1024, d=128
  - T=2048, d=256

### åŸºç¡€ç®—å­æµ‹è¯•

#### 4. Linear Layer (2ä¸ªæµ‹è¯•)
```python
y = xA^T + b
```
- å®¹å·®: rtol=1e-4, atol=1e-5

#### 5. RMS Norm (2ä¸ªæµ‹è¯•)
```python
output = input / sqrt(mean(input^2) + eps) * weight
```
- å®¹å·®: rtol=1e-4, atol=1e-5

#### 6. Embedding (2ä¸ªæµ‹è¯•)
```python
output = weight[indices]
```
- å®¹å·®: rtol=1e-5, atol=1e-6

#### 7. Argmax (2ä¸ªæµ‹è¯•)
```python
output = argmax(input, dim=-1)
```
- ç²¾ç¡®åŒ¹é…ï¼ˆæ•´æ•°ç´¢å¼•ï¼‰

#### 8. SwiGLU (2ä¸ªæµ‹è¯•)
```python
output = silu(gate) * x
```
- å®¹å·®: rtol=1e-5, atol=1e-6

## âœ… éªŒè¯æµç¨‹

### Pythonä¾§å·¥ä½œæµ

```python
# 1. PyTorchValidatorç±» - å‚è€ƒå®ç°
class PyTorchValidator:
    @staticmethod
    def scaled_dot_product_attention(q, k, v):
        return torch.nn.functional.scaled_dot_product_attention(q, k, v)

# 2. TestGeneratorç±» - ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
generator = TestGenerator()
generator.generate_all_tests()  # ç”Ÿæˆ18ä¸ªæµ‹è¯•

# 3. check_resultså‡½æ•° - éªŒè¯ç»“æœ
check_results()  # æ¯”è¾ƒC++è¾“å‡ºå’ŒPyTorchå‚è€ƒ
```

### C++ä¾§å·¥ä½œæµ

```cpp
// 1. åŠ è½½æµ‹è¯•æ•°æ®
auto q_data = load_npy_float32("test_data/self_attention_1_query.npy");
auto k_data = load_npy_float32("test_data/self_attention_1_key.npy");
auto v_data = load_npy_float32("test_data/self_attention_1_value.npy");

// 2. åˆ›å»ºtensorå¹¶è¿è¡Œç®—å­
TensorF q(q_data, Shape({2, 2, 8, 16}));
TensorF k(k_data, Shape({2, 2, 8, 16}));
TensorF v(v_data, Shape({2, 2, 8, 16}));
TensorF output = self_attention(q, k, v, nullptr, scale);

// 3. ä¿å­˜è¾“å‡º
save_npy_float32("test_data/cpp_self_attention_1_output.npy",
                  output.data(), output.size(), shape);
```

## ğŸ“Š å®¹å·®è®¾ç½®è¯´æ˜

ä¸åŒç®—å­ä½¿ç”¨ä¸åŒçš„å®¹å·®ï¼ŒåŸºäºæ•°å€¼ç¨³å®šæ€§è€ƒè™‘ï¼š

| ç®—å­ç±»å‹ | ç›¸å¯¹å®¹å·®(rtol) | ç»å¯¹å®¹å·®(atol) | è¯´æ˜ |
|---------|---------------|---------------|------|
| Attention | 1e-3 | 1e-4 | softmax+matmulç´¯ç§¯è¯¯å·® |
| Linear | 1e-4 | 1e-5 | å•æ¬¡matmul |
| RMS Norm | 1e-4 | 1e-5 | å¹³æ–¹æ ¹æ“ä½œ |
| Embedding | 1e-5 | 1e-6 | ç®€å•æŸ¥è¡¨ |
| SwiGLU | 1e-5 | 1e-6 | SiLUæ¿€æ´»å‡½æ•° |
| Argmax | 0 | 0 | æ•´æ•°ç²¾ç¡®åŒ¹é… |

## ğŸ”§ æ‰©å±•æ–°ç®—å­

### æ·»åŠ æ–°ç®—å­éªŒè¯çš„æ­¥éª¤

1. **åœ¨PyTorchValidatorä¸­æ·»åŠ å‚è€ƒå®ç°**
```python
class PyTorchValidator:
    @staticmethod
    def your_new_operator(input1, input2):
        # ä½¿ç”¨PyTorchå®ç°
        return torch.some_function(input1, input2)
```

2. **åœ¨TestGeneratorä¸­æ·»åŠ æµ‹è¯•ç”Ÿæˆå™¨**
```python
def generate_your_operator_tests(self):
    validator = PyTorchValidator()
    config = {'param1': 64, 'param2': 128}

    # ç”Ÿæˆè¾“å…¥
    input1 = torch.randn(...)
    input2 = torch.randn(...)

    # è®¡ç®—å‚è€ƒ
    ref = validator.your_new_operator(input1, input2)

    # ä¿å­˜æµ‹è¯•ç”¨ä¾‹
    test_case = TestCase(
        name=f"your_operator_1",
        inputs={'input1': input1.numpy(), 'input2': input2.numpy()},
        reference_output=ref.numpy(),
        tolerance={'rtol': 1e-4, 'atol': 1e-5}
    )
    self.save_test_case(test_case)
```

3. **åœ¨torch_validation.cppä¸­æ·»åŠ è¿è¡Œå™¨**
```cpp
void run_your_operator_test(const string& test_name, const string& data_dir) {
    // åŠ è½½è¾“å…¥
    auto input1 = load_npy_float32(data_dir + "/" + test_name + "_input1.npy");
    auto input2 = load_npy_float32(data_dir + "/" + test_name + "_input2.npy");

    // è¿è¡Œç®—å­
    TensorF output = your_operator(input1, input2);

    // ä¿å­˜è¾“å‡º
    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy", ...);
}
```

4. **åœ¨generate_all_testsä¸­æ³¨å†Œ**
```python
def generate_all_tests(self):
    # ... ç°æœ‰æµ‹è¯• ...
    self.generate_your_operator_tests()
```

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: ImportError: No module named 'torch'**
```bash
pip install torch numpy
```

**Q: numpyåŠ è½½å¤±è´¥**
```bash
pip install --upgrade numpy
```

**Q: C++ç¼–è¯‘é”™è¯¯**
```bash
# ç¡®ä¿å®‰è£…äº†å¿…è¦çš„ä¾èµ–
sudo apt install libomp-dev libopenmpi-dev
```

**Q: æµ‹è¯•å¤±è´¥ä½†è¯¯å·®å¾ˆå°**
- æ£€æŸ¥å®¹å·®è®¾ç½®æ˜¯å¦åˆç†
- æµ®ç‚¹è¿ç®—åœ¨ä¸åŒå¹³å°å¯èƒ½æœ‰å¾®å°å·®å¼‚
- å¯ä»¥é€‚å½“è°ƒæ•´å®¹å·®

## ğŸ“ˆ CI/CDé›†æˆ

ç¤ºä¾‹GitHub Actionsé…ç½®ï¼š

```yaml
name: PyTorch Validation

on: [push, pull_request]

jobs:
  validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install dependencies
        run: |
          pip install torch numpy
      - name: Generate test data
        run: python3 torch_validation.py
      - name: Build C++ tests
        run: |
          cd tensor_cpp
          make torch-validation
      - name: Run C++ validation
        run: |
          cd tensor_cpp
          ./build/torch_validation
      - name: Check results
        run: python3 torch_validation.py --check-results
```

## ğŸ“ å­¦ä¹ èµ„æº

- [PyTorch Attentionæœºåˆ¶](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
- [Online Softmaxç®—æ³•](https://arxiv.org/abs/2002.05702)
- [Flash Attention](https://arxiv.org/abs/2205.14135)

## ğŸ“ æ›´æ–°æ—¥å¿—

- **2026-01-11**: åˆå§‹ç‰ˆæœ¬
  - å®ç°18ä¸ªæµ‹è¯•ç”¨ä¾‹
  - è¦†ç›–8ç±»ç®—å­
  - æ”¯æŒå®Œæ•´éªŒè¯æµç¨‹

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤æ–°çš„ç®—å­éªŒè¯æµ‹è¯•ï¼

---

**æ³¨æ„**: æ‰€æœ‰æµ‹è¯•ä½¿ç”¨å›ºå®šéšæœºç§å­(torch.manual_seed(42))ä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚
