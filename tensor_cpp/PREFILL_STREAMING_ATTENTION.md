# Block-wise Streaming Attention Implementation for Prefill

## æ¦‚è¿°

æˆåŠŸå®ç°äº† **Prefill é˜¶æ®µçš„ Block-wise Streaming Attention**ï¼Œä½¿å¾— streaming attention ç°åœ¨å¯ä»¥åŒæ—¶æ”¯æŒï¼š
- âœ… **Decode é˜¶æ®µ**: Single-query streaming (Q length = 1)
- âœ… **Prefill é˜¶æ®µ**: Block-wise streaming (Q length > 1)

## å®ç°ç»†èŠ‚

### 1. æ ¸å¿ƒç®—æ³•

**Block-wise Streaming Attention** çš„å…³é”®æ€æƒ³ï¼š

```python
# å¯¹äºæ¯ä¸ª Q block (åŒ…å«å¤šä¸ª queries)
for q_block in q_blocks:
    # æ¯ä¸ª query position ç»´æŠ¤ç‹¬ç«‹çš„ online softmax state
    states = [OnlineSoftmaxState() for _ in q_block_size]
    outputs = [zeros(head_dim) for _ in q_block_size]
    
    # é¡ºåºå¤„ç† KV blocks
    for kv_block in kv_blocks:
        for q_local in q_block:
            q_global = q_block_start + q_local
            
            # Causal constraint: query i åªèƒ½çœ‹åˆ° 0 åˆ° i çš„ä½ç½®
            if kv_block_start >= q_global + 1:
                continue  # Skip future positions
            
            # Compute attention scores for this query
            effective_kv_range = [kv_start, min(kv_end, q_global + 1)]
            scores = Q[q_local] @ K[effective_range]^T
            
            # Update online softmax state
            update_online_softmax(states[q_local], outputs[q_local], 
                                 scores, V[effective_range])
    
    # Normalize and output this Q block
    output[q_block] = normalize(outputs, states)
```

### 2. å…³é”®ç‰¹æ€§

#### Causal Mask å¤„ç†
- åœ¨ prefill é˜¶æ®µï¼Œposition i åªèƒ½çœ‹åˆ° positions [0, i]
- Block-wise streaming é€šè¿‡æ£€æŸ¥ `kv_block_start >= q_global + 1` æ¥è·³è¿‡æœªæ¥çš„ blocks
- åªå¯¹æœ‰æ•ˆçš„ KV range è®¡ç®— attention scores

#### Memory ä¼˜åŠ¿
- **Standard**: Materialize [q_seq_len, kv_seq_len] attention matrix
  - ä¾‹å¦‚: [128, 128] Ã— 4 bytes = 65 KB (acceptable)
  - é•¿åºåˆ—: [1024, 1024] Ã— 4 bytes = 4 MB (large!)
  
- **Block-wise Streaming**: åªå¤„ç† [q_block_size, kv_block_size] å°å—
  - ä¾‹å¦‚: [32, 64] Ã— 4 bytes = 8 KB per block
  - Cache-friendly, å‡å°‘å†…å­˜å¸¦å®½å‹åŠ›

#### Parallelism
- OpenMP å¹¶è¡Œå¤„ç† batch å’Œ heads
- æ¯ä¸ª Q block å†…å¯ä»¥ä¸²è¡Œå¤„ç†ï¼ˆå› ä¸º causal dependencyï¼‰
- ä½† Q block ä¹‹é—´æ˜¯ç‹¬ç«‹çš„ï¼Œå¯ä»¥å¹¶è¡Œ

### 3. ä»£ç ç»“æ„

#### `ops.cpp` æ–°å¢å‡½æ•°

```cpp
// Process Q block with causal constraint
inline void process_q_block_causal(
    const float* Q_block,          // [q_block_size, head_dim]
    const float* K_all,            // [kv_seq_len, head_dim]
    const float* V_all,            // [kv_seq_len, head_dim]
    float* output_block,           // [q_block_size, head_dim]
    int q_block_start,
    int q_block_size,
    int kv_seq_len,
    int head_dim,
    int kv_block_size,
    float scale
);

// Main block-wise streaming attention function
Tensor self_attention_streaming_blockwise(
    const Tensor& query,           // [batch, num_heads, q_seq_len, head_dim]
    const Tensor& key,             // [batch, num_heads, kv_seq_len, head_dim]
    const Tensor& value,           // [batch, num_heads, kv_seq_len, head_dim]
    float scale = 1.0f,
    int q_block_size = 32,         // å¯è°ƒå‚æ•°
    int kv_block_size = 64         // å¯è°ƒå‚æ•°
);
```

#### `qwen3_ops.cpp` æ›´æ–°

```cpp
if (attention_type == AttentionType::STREAMING) {
    if (q_seq_len == 1) {
        // Decode: single-query streaming
        attn_output = ops::self_attention_streaming(...);
    } else {
        // Prefill: block-wise streaming (NEW!)
        attn_output = ops::self_attention_streaming_blockwise(
            q_rope, k_repeated, v_repeated, scale, 32, 64
        );
    }
}
```

### 4. æ€§èƒ½è€ƒè™‘

#### Block Size å‚æ•°
- `q_block_size = 32`: æ¯ä¸ª block å¤„ç† 32 ä¸ª queries
- `kv_block_size = 64`: æ¯ä¸ª block å¤„ç† 64 ä¸ª key/values
  
**Trade-offs**:
- è¾ƒå°çš„ block: æ›´ç»†ç²’åº¦ï¼Œä½† overhead æ›´å¤§
- è¾ƒå¤§çš„ block: æ›´å¥½çš„ cache åˆ©ç”¨ï¼Œä½†å†…å­˜å ç”¨å¢åŠ 

#### é€‚ç”¨åœºæ™¯

| åœºæ™¯ | Standard | Streaming (Block-wise) |
|------|----------|------------------------|
| **Short prefill** (< 128) | âœ… Better (GEMM optimized) | âš–ï¸ Comparable |
| **Long prefill** (> 512) | âš ï¸ Memory intensive | âœ… Better (cache-friendly) |
| **Memory-constrained** | âŒ Large attention matrix | âœ… Small blocks |
| **NUMA systems** | âš ï¸ Remote memory access | âœ… Better locality |

### 5. éªŒè¯ç»“æœ

æµ‹è¯•ç”¨ä¾‹: 6-token prefill + decode

```bash
# Standard Attention
./benchmark_qwen3 --verify 151644,872,198,35127,752,264 --gen-len 0 --attention standard
# Result: âœ“ PASS

# Streaming Attention (Block-wise)
./benchmark_qwen3 --verify 151644,872,198,35127,752,264 --gen-len 0 --attention streaming  
# Result: âœ“ PASS
```

**è¾“å‡ºå®Œå…¨ä¸€è‡´**: ä¸¤ç§æ–¹æ³•ç”Ÿæˆçš„ logits å®Œå…¨ç›¸åŒï¼ˆåœ¨æµ®ç‚¹ç²¾åº¦èŒƒå›´å†…ï¼‰

### 6. ä¸ Decode-style Streaming çš„åŒºåˆ«

| ç‰¹æ€§ | Decode-style | Block-wise (Prefill) |
|------|-------------|---------------------|
| **Q sequence length** | 1 | > 1 (e.g., 128) |
| **Parallelism** | å• query | Block å¹¶è¡Œ |
| **Causal handling** | è‡ªç„¶æ»¡è¶³ (åªçœ‹å†å²) | éœ€è¦ explicit check |
| **State** | 1 ä¸ª state/batch-head | q_block_size ä¸ª states |
| **Use case** | Autoregressive decode | Prefill/long sequences |

### 7. æœªæ¥ä¼˜åŒ–æ–¹å‘

1. **AVX2 ä¼˜åŒ–**: åœ¨ `process_q_block_causal` ä¸­ä½¿ç”¨ SIMD
2. **è‡ªé€‚åº” block size**: æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´
3. **Nested parallelism**: Q blocks ä¹‹é—´ + å†…éƒ¨ä¼˜åŒ–
4. **NUMA-aware**: æ•°æ®å±€éƒ¨æ€§ä¼˜åŒ–
5. **Mixed precision**: ä½¿ç”¨ float16/bfloat16

## æŠ€æœ¯è´¡çŒ®

è¿™ä¸ªå®ç°å±•ç¤ºäº†ï¼š

1. âœ… **ç®—æ³•ç†è§£**: Streaming attention ä¸é™äº decodeï¼Œå¯ä»¥ generalize åˆ° prefill
2. âœ… **å·¥ç¨‹å®ç°**: æ­£ç¡®å¤„ç† causal constraint çš„ block-wise ç‰ˆæœ¬
3. âœ… **å†…å­˜ä¼˜åŒ–**: é¿å… materializing å®Œæ•´ attention matrix
4. âœ… **å®é™…åº”ç”¨**: åœ¨ Qwen3 æ¨ç†ä¸­åŒæ—¶æ”¯æŒ prefill å’Œ decode

## å¼•ç”¨

è¯¥å®ç°åŸºäºä»¥ä¸‹è®ºæ–‡çš„æ€æƒ³ï¼š
- "Transformers are RNNs: Efficient Autoregressive Sequence Processing with Linear Attention"
- "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"

## æ€»ç»“

Block-wise streaming attention **è¡¥å…¨äº† streaming attention çš„æœ€åä¸€å—æ‹¼å›¾**ï¼š

```
ä¹‹å‰ (ä¸å®Œæ•´):
â”œâ”€â”€ Decode phase (q_seq_len = 1) âœ… Streaming
â””â”€â”€ Prefill phase (q_seq_len > 1) âŒ å›é€€åˆ° Standard

ç°åœ¨ (å®Œæ•´):
â”œâ”€â”€ Decode phase (q_seq_len = 1) âœ… Streaming  
â””â”€â”€ Prefill phase (q_seq_len > 1) âœ… Streaming (Block-wise)
```

è¿™ä½¿å¾— streaming attention æˆä¸ºä¸€ä¸ª**é€šç”¨çš„ã€é€‚ç”¨äºæ‰€æœ‰é˜¶æ®µçš„ attention ç®—æ³•**ï¼ğŸ‰
