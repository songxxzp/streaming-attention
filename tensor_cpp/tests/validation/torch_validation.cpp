/**
 * @file torch_validation.cpp
 * @brief C++ validation against PyTorch reference outputs
 *
 * This program loads test data generated by torch_validation.py,
 * runs the tensor_cpp operators, and saves outputs for comparison.
 *
 * Build:
 *   cd tensor_cpp && make torch-validation
 *
 * Run:
 *   ./build/torch_validation
 */

#include "tensor_cpp/tensor.h"
#include "tensor_cpp/ops.h"
#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <cstring>

using namespace tensor_cpp;
using namespace tensor_cpp::ops;

// Simple JSON parser (minimal implementation for test metadata)
struct TestMetadata {
    std::string name;
    std::vector<std::string> input_files;
    std::string reference_file;
    float rtol = 1e-3f;
    float atol = 1e-5f;
};

// Load numpy array from .npy file (simplified - assumes C order and float32)
std::vector<float> load_npy_float32(const std::string& path, std::vector<int>* shape = nullptr) {
    FILE* fp = fopen(path.c_str(), "rb");
    if (!fp) {
        throw std::runtime_error("Failed to open file: " + path);
    }

    // Skip header (simplified - just find the data)
    char header[256];
    size_t header_len = 0;
    char c;
    while (fread(&c, 1, 1, fp) == 1 && header_len < 255) {
        header[header_len++] = c;
        if (c == '\n') break;  // End of first line
    }
    header[header_len] = '\0';

    // Skip rest of header until '\n'
    while (fread(&c, 1, 1, fp) == 1) {
        if (c == '\n') break;
    }

    // Read remaining data
    fseek(fp, 0, SEEK_END);
    long file_size = ftell(fp);
    long data_offset = ftell(fp);

    fseek(fp, 0, SEEK_END);
    file_size = ftell(fp);

    // Go back to data (skip header properly - this is simplified)
    fseek(fp, 0, SEEK_SET);
    while (fread(&c, 1, 1, fp) == 1) {
        static int newline_count = 0;
        if (c == '\n') {
            newline_count++;
            if (newline_count >= 2) break;
        }
    }

    // Read all remaining floats
    std::vector<float> data;
    float value;
    while (fread(&value, sizeof(float), 1, fp) == 1) {
        data.push_back(value);
    }

    fclose(fp);
    return data;
}

// Load int64 array
std::vector<long> load_npy_int64(const std::string& path) {
    FILE* fp = fopen(path.c_str(), "rb");
    if (!fp) {
        throw std::runtime_error("Failed to open file: " + path);
    }

    // Skip header
    char c;
    int newline_count = 0;
    while (fread(&c, 1, 1, fp) == 1) {
        if (c == '\n') {
            newline_count++;
            if (newline_count >= 2) break;
        }
    }

    // Read int64 values
    std::vector<long> data;
    int64_t value;
    while (fread(&value, sizeof(int64_t), 1, fp) == 1) {
        data.push_back(static_cast<long>(value));
    }

    fclose(fp);
    return data;
}

// Save output as .npy file
void save_npy_float32(const std::string& path, const float* data, size_t size, const std::vector<int>& shape) {
    FILE* fp = fopen(path.c_str(), "wb");
    if (!fp) {
        throw std::runtime_error("Failed to create file: " + path);
    }

    // Write simple header
    std::string header = "\x93NUMPY\x01\x00v\x00";  // Magic, version, header len
    header += "{'descr': '<f4', 'fortran_order': False, 'shape': (";
    for (size_t i = 0; i < shape.size(); ++i) {
        header += std::to_string(shape[i]);
        if (i < shape.size() - 1) header += ", ";
    }
    header += "), }";
    while (header.size() % 64 != 63) header += " ";
    header += "\n";

    fwrite(header.data(), 1, header.size(), fp);
    fwrite(data, sizeof(float), size, fp);
    fclose(fp);
}

void save_npy_int64(const std::string& path, const long* data, size_t size, const std::vector<int>& shape) {
    FILE* fp = fopen(path.c_str(), "wb");
    if (!fp) {
        throw std::runtime_error("Failed to create file: " + path);
    }

    std::string header = "\x93NUMPY\x01\x00v\x00";
    header += "{'descr': '<i8', 'fortran_order': False, 'shape': (";
    for (size_t i = 0; i < shape.size(); ++i) {
        header += std::to_string(shape[i]);
        if (i < shape.size() - 1) header += ", ";
    }
    header += "), }";
    while (header.size() % 64 != 63) header += " ";
    header += "\n";

    fwrite(header.data(), 1, header.size(), fp);
    fwrite(data, sizeof(int64_t), size, fp);
    fclose(fp);
}

// Test runner
struct ValidationResult {
    bool passed;
    float max_abs_error;
    float max_rel_error;
    std::string message;
};

void run_self_attention_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    // Load inputs
    std::vector<float> q_data = load_npy_float32(data_dir + "/" + test_name + "_query.npy");
    std::vector<float> k_data = load_npy_float32(data_dir + "/" + test_name + "_key.npy");
    std::vector<float> v_data = load_npy_float32(data_dir + "/" + test_name + "_value.npy");
    std::vector<float> scale_data = load_npy_float32(data_dir + "/" + test_name + "_scale.npy");
    float scale = scale_data[0];

    // Determine shape from data size
    // Assuming [batch, heads, seq, dim] format
    size_t total_q = q_data.size();
    size_t total_v = v_data.size();

    // Infer dimensions (simplified - assumes batch*heads=4 for small tests)
    int dim = 16;  // Will be inferred from actual test
    if (total_q == 512) { dim = 16; }  // 2*2*8*16
    else if (total_q == 2048) { dim = 32; }  // 1*4*16*32
    else if (total_q == 131072) { dim = 64; }  // 4*8*64*64

    // Create tensors
    TensorF q(q_data, Shape({2, 2, 8, dim}));
    TensorF k(k_data, Shape({2, 2, 8, dim}));
    TensorF v(v_data, Shape({2, 2, 8, dim}));

    // Run operator
    TensorF output = tensor_cpp::ops::self_attention(q, k, v, nullptr, scale);

    // Save output
    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1]),
                      static_cast<int>(output.shape()[2]),
                      static_cast<int>(output.shape()[3])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_cross_attention_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> q_data = load_npy_float32(data_dir + "/" + test_name + "_query.npy");
    std::vector<float> k_data = load_npy_float32(data_dir + "/" + test_name + "_key.npy");
    std::vector<float> v_data = load_npy_float32(data_dir + "/" + test_name + "_value.npy");
    std::vector<float> scale_data = load_npy_float32(data_dir + "/" + test_name + "_scale.npy");
    float scale = scale_data[0];

    // Create tensors (cross attention has different seq lengths)
    TensorF q(q_data, Shape({2, 2, 8, 16}));
    TensorF k(k_data, Shape({2, 2, 16, 16}));
    TensorF v(v_data, Shape({2, 2, 16, 16}));

    TensorF output = tensor_cpp::ops::cross_attention(q, k, v, nullptr, scale);

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1]),
                      static_cast<int>(output.shape()[2]),
                      static_cast<int>(output.shape()[3])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_linear_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> x_data = load_npy_float32(data_dir + "/" + test_name + "_input.npy");
    std::vector<float> w_data = load_npy_float32(data_dir + "/" + test_name + "_weight.npy");
    std::vector<float> b_data = load_npy_float32(data_dir + "/" + test_name + "_bias.npy");

    TensorF x(x_data, Shape({static_cast<long>(x_data.size() / w_data.size()),
                             static_cast<long>(w_data.size())}));
    TensorF weight(w_data, Shape({static_cast<long>(w_data.size()),
                                  static_cast<long>(x_data.size() / w_data.size())}));
    TensorF bias(b_data, Shape({static_cast<long>(b_data.size())}));

    TensorF output = tensor_cpp::ops::linear(x, weight, &bias);

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_rms_norm_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> x_data = load_npy_float32(data_dir + "/" + test_name + "_input.npy");
    std::vector<float> w_data = load_npy_float32(data_dir + "/" + test_name + "_weight.npy");

    TensorF x(x_data, Shape({2, 16, static_cast<long>(w_data.size())}));
    TensorF weight(w_data, Shape({static_cast<long>(w_data.size())}));

    TensorF output = tensor_cpp::ops::rms_norm(x, &weight);

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1]),
                      static_cast<int>(output.shape()[2])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_embedding_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<long> indices_data = load_npy_int64(data_dir + "/" + test_name + "_indices.npy");
    std::vector<float> weight_data = load_npy_float32(data_dir + "/" + test_name + "_weight.npy");

    TensorL indices(indices_data, Shape({static_cast<long>(indices_data.size())}));
    TensorF weight(weight_data, Shape({static_cast<long>(weight_data.size() / weight_data.size()),
                                       static_cast<long>(weight_data.size())}));

    // Infer shape from weight
    long num_embeddings = weight_data.size() / 64;  // Simplified
    long embedding_dim = 64;
    TensorF weight_reshaped(weight_data, Shape({num_embeddings, embedding_dim}));

    TensorF output = tensor_cpp::ops::embedding(indices, weight_reshaped);

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_argmax_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> x_data = load_npy_float32(data_dir + "/" + test_name + "_input.npy");

    TensorF x(x_data, Shape({2, static_cast<long>(x_data.size() / 2)}));

    TensorL output = tensor_cpp::ops::argmax(x, -1, false);

    save_npy_int64(data_dir + "/cpp_" + test_name + "_output.npy",
                   output.data(), output.size(),
                   {static_cast<int>(output.size())});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_swiglu_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> x_data = load_npy_float32(data_dir + "/" + test_name + "_x.npy");
    std::vector<float> gate_data = load_npy_float32(data_dir + "/" + test_name + "_gate.npy");

    TensorF x(x_data, Shape({2, 16, static_cast<long>(x_data.size() / 32)}));
    TensorF gate(gate_data, Shape({2, 16, static_cast<long>(gate_data.size() / 32)}));

    TensorF output = tensor_cpp::ops::swiglu(x, gate);

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.shape()[0]),
                      static_cast<int>(output.shape()[1]),
                      static_cast<int>(output.shape()[2])});

    std::cout << "  ✓ Saved output for validation\n";
}

void run_streaming_attention_test(const std::string& test_name, const std::string& data_dir) {
    std::cout << "Testing: " << test_name << "\n";

    std::vector<float> Q_data = load_npy_float32(data_dir + "/" + test_name + "_Q.npy");
    std::vector<float> K_data = load_npy_float32(data_dir + "/" + test_name + "_K.npy");
    std::vector<float> V_data = load_npy_float32(data_dir + "/" + test_name + "_V.npy");
    std::vector<float> T_data = load_npy_float32(data_dir + "/" + test_name + "_T.npy");
    std::vector<float> d_data = load_npy_float32(data_dir + "/" + test_name + "_d.npy");

    int T = static_cast<int>(T_data[0]);
    int d = static_cast<int>(d_data[0]);

    std::vector<float> output = tensor_cpp::ops::streaming_attention_serial(
        Q_data.data(), K_data.data(), V_data.data(), T, d, 64
    );

    save_npy_float32(data_dir + "/cpp_" + test_name + "_output.npy",
                     output.data(), output.size(),
                     {static_cast<int>(output.size())});

    std::cout << "  ✓ Saved output for validation\n";
}

int main(int argc, char** argv) {
    std::string data_dir = "test_data";

    if (argc > 1) {
        data_dir = argv[1];
    }

    std::cout << "\n";
    std::cout << "========================================================\n";
    std::cout << "  C++ Validation Against PyTorch Reference\n";
    std::cout << "========================================================\n\n";

    std::cout << "Test data directory: " << data_dir << "\n\n";

    try {
        // Run all tests
        run_self_attention_test("self_attention_1", data_dir);
        run_self_attention_test("self_attention_2", data_dir);
        run_self_attention_test("self_attention_3", data_dir);

        run_cross_attention_test("cross_attention_1", data_dir);
        run_cross_attention_test("cross_attention_2", data_dir);

        run_streaming_attention_test("streaming_attention_1", data_dir);
        run_streaming_attention_test("streaming_attention_2", data_dir);
        run_streaming_attention_test("streaming_attention_3", data_dir);

        run_linear_test("linear_1", data_dir);
        run_linear_test("linear_2", data_dir);

        run_rms_norm_test("rms_norm_1", data_dir);
        run_rms_norm_test("rms_norm_2", data_dir);

        run_embedding_test("embedding_1", data_dir);
        run_embedding_test("embedding_2", data_dir);

        run_argmax_test("argmax_1", data_dir);
        run_argmax_test("argmax_2", data_dir);

        run_swiglu_test("swiglu_1", data_dir);
        run_swiglu_test("swiglu_2", data_dir);

        std::cout << "\n";
        std::cout << "========================================================\n";
        std::cout << "  All C++ Outputs Saved Successfully\n";
        std::cout << "========================================================\n\n";
        std::cout << "To validate results, run:\n";
        std::cout << "  python torch_validation.py --check-results\n\n";

        return 0;

    } catch (const std::exception& e) {
        std::cerr << "\n✗ ERROR: " << e.what() << "\n\n";
        return 1;
    }
}
